{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e19105c",
   "metadata": {},
   "source": [
    "# Notebook 4 - Graph Analysis Fraud Detection (Reconstruit pour Spark 4.0.1)\n",
    "\n",
    "**Objectif :** Simuler la d√©tection de fraude par analyse de graphe (PageRank) en utilisant uniquement les fonctionnalit√©s natives de **PySpark DataFrames** (`pyspark.sql`) pour contourner l'incompatibilit√© de la librairie GraphFrames avec Spark 4.0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7280e7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session (version 4.0.1) d√©marr√©e sans GraphFrames.\n"
     ]
    }
   ],
   "source": [
    "# üîπ √âtape 1 : Initialisation Spark et Imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, sum, explode\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fraud-Graph-Sim\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Session (version {spark.version}) d√©marr√©e sans GraphFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "957ceb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chargement du CSV original ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=================================>                       (7 + 5) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donn√©es charg√©es.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 01:31:18 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    }
   ],
   "source": [
    "# üîπ √âtape 2 : Charger le dataset (Utilisation du CSV original)\n",
    "print(\"--- Chargement du CSV original ---\")\n",
    "df = spark.read.csv(\n",
    "    \"hdfs:///user/hadoop/BigDataFraude_ML-GraphX/creditcard.csv\", \n",
    "    header=True, \n",
    "    inferSchema=True\n",
    ")\n",
    "df.cache()\n",
    "print(\"Donn√©es charg√©es.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef248b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cr√©ation des Vertices (Clients V1) ---\n",
      "+----------------+-----+------+--------+\n",
      "|              id|label|Amount|pagerank|\n",
      "+----------------+-----+------+--------+\n",
      "|-1.2768303373631|    0| 110.4|     1.0|\n",
      "|1.21205680491093|    0|  2.28|     1.0|\n",
      "|1.08102680841932|    0| 17.24|     1.0|\n",
      "|1.49157444507907|    0|   2.0|     1.0|\n",
      "|1.09337038677875|    0|  49.9|     1.0|\n",
      "+----------------+-----+------+--------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 01:31:19 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# üîπ √âtape 3 : Cr√©er les Vertices (Noeuds)\n",
    "# Utiliser V1 (qui simule le client/carte) comme ID.\n",
    "print(\"--- Cr√©ation des Vertices (Clients V1) ---\")\n",
    "vertices = df.select(\n",
    "    col(\"V1\").alias(\"id\"),  \n",
    "    col(\"Class\").alias(\"label\"),\n",
    "    col(\"Amount\")\n",
    ").distinct()\n",
    "\n",
    "# Ajout d'une colonne pour le PageRank initial\n",
    "vertices = vertices.withColumn(\"pagerank\", lit(1.0))\n",
    "vertices.cache()\n",
    "vertices.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db3225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 01:31:19 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cr√©ation des Edges (Liens Frauduleux Co-occurrents) ---\n",
      "+-----------------+-----------------+\n",
      "|              src|              dst|\n",
      "+-----------------+-----------------+\n",
      "|0.314596589729515|-1.58550536691994|\n",
      "|-4.72771265581559|-2.58961719821269|\n",
      "|-16.5986647432584|-25.2663550194138|\n",
      "|-19.8563223334433|  -27.84818067198|\n",
      "|-2.78724793061533|  -27.84818067198|\n",
      "+-----------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# üîπ √âtape 4 : Cr√©er les Edges (Liens)\n",
    "# Lien cr√©√© si deux clients diff√©rents (V1) font une transaction frauduleuse (Class=1) dans la m√™me heure.\n",
    "print(\"--- Cr√©ation des Edges (Liens Frauduleux Co-occurrents) ---\")\n",
    "edges = df.alias(\"t1\").join(\n",
    "    df.alias(\"t2\"),\n",
    "    (\n",
    "        # 1. Les deux sont des fraudes\n",
    "        (col(\"t1.Class\") == 1) & (col(\"t2.Class\") == 1) &\n",
    "        \n",
    "        # 2. Elles se produisent dans la m√™me heure (Correction syntaxique)\n",
    "        ((col(\"t1.Time\") / 3600).cast(\"int\") == (col(\"t2.Time\") / 3600).cast(\"int\")) &\n",
    "        \n",
    "        # 3. Ce sont des entit√©s/clients diff√©rents (src != dst)\n",
    "        (col(\"t1.V1\") != col(\"t2.V1\"))\n",
    "    )\n",
    ").select(\n",
    "    col(\"t1.V1\").alias(\"src\"),\n",
    "    col(\"t2.V1\").alias(\"dst\")\n",
    ").distinct()\n",
    "edges.cache()\n",
    "edges.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2760b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pr√©paration : Calcul des degr√©s sortants ---\n",
      "+-----------------+-----------------+------+\n",
      "|              src|              dst|weight|\n",
      "+-----------------+-----------------+------+\n",
      "|-14.4744374924863|-16.3679230107968|   0.1|\n",
      "|-14.4744374924863|-15.3988450085358|   0.1|\n",
      "|-14.4744374924863|-12.2240206243564|   0.1|\n",
      "|-14.4744374924863|-15.2713618637585|   0.1|\n",
      "|-14.4744374924863|-14.1791651073631|   0.1|\n",
      "+-----------------+-----------------+------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 01:31:19 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# üîπ √âtape 5 : Calcul du degr√© de sortie et normalisation (Pr√©paration PageRank)\n",
    "# Pour PageRank, nous avons besoin du nombre de liens sortants par source.\n",
    "print(\"--- Pr√©paration : Calcul des degr√©s sortants ---\")\n",
    "out_degrees = edges.groupBy(\"src\")\\\n",
    "                   .count()\\\n",
    "                   .withColumnRenamed(\"count\", \"outDegree\")\n",
    "\n",
    "# Joindre les degrees aux edges pour la normalisation\n",
    "normalized_edges = edges.join(out_degrees, \"src\")\\\n",
    "                        .withColumn(\"weight\", lit(1.0) / col(\"outDegree\"))\\\n",
    "                        .select(\"src\", \"dst\", \"weight\")\n",
    "normalized_edges.cache()\n",
    "normalized_edges.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c637340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PageRank Simul√© (5 it√©rations) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 01:32:06 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It√©ration 1 compl√©t√©e.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It√©ration 2 compl√©t√©e.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It√©ration 3 compl√©t√©e.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It√©ration 4 compl√©t√©e.\n",
      "It√©ration 5 compl√©t√©e.\n",
      "+-------------------+-----+------------------+\n",
      "|                 id|label|          pagerank|\n",
      "+-------------------+-----+------------------+\n",
      "|  -1.58550536691994|    1|0.4437073305244993|\n",
      "|  -3.49910753739178|    1|0.4437073305244993|\n",
      "|  -0.25147096006823|    1|0.4437073305244993|\n",
      "|0.00843036489558254|    1|0.4437073305244993|\n",
      "|  -1.81328048476897|    1|0.4437073305244993|\n",
      "|  0.725645739819857|    1|0.4437073305244993|\n",
      "| 0.0267792264491516|    1|0.4437073305244993|\n",
      "|  0.314596589729515|    1|0.4437073305244993|\n",
      "|  0.857321003765953|    1|0.4437073305244993|\n",
      "|  -1.78322883722709|    1|0.4437073305244993|\n",
      "+-------------------+-----+------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# üîπ √âtape 6 : Algorithme PageRank (Simul√© avec PySpark DataFrames)\n",
    "\n",
    "from pyspark.sql.functions import coalesce # Importation requise pour la correction\n",
    "\n",
    "MAX_ITER = 5\n",
    "RESET_PROBABILITY = 0.15\n",
    "\n",
    "print(f\"--- PageRank Simul√© ({MAX_ITER} it√©rations) ---\")\n",
    "\n",
    "current_vertices = vertices.select(\"id\", \"label\", \"pagerank\").cache()\n",
    "\n",
    "for i in range(MAX_ITER):\n",
    "    # 1. Calcul du Score de contribution de chaque source (pr * weight)\n",
    "    contributions = current_vertices.join(\n",
    "        normalized_edges, current_vertices.id == normalized_edges.src\n",
    "    ).withColumn(\n",
    "        \"contribution\", col(\"pagerank\") * col(\"weight\")\n",
    "    ).select(col(\"dst\").alias(\"id\"), \"contribution\")\n",
    "    \n",
    "    # 2. Agr√©gation des contributions par destination (nouveau PageRank)\n",
    "    new_pageranks = contributions.groupBy(\"id\").agg(sum(\"contribution\").alias(\"sum_contribution\"))\n",
    "    \n",
    "    # 3. Application du facteur de r√©initialisation (PageRank Formula)\n",
    "    current_vertices = current_vertices.drop(\"pagerank\")\\\n",
    "                                       .join(new_pageranks, \"id\", \"left_outer\")\\\n",
    "                                       .withColumn(\n",
    "                                           \"sum_contribution_clean\", \n",
    "                                           coalesce(col(\"sum_contribution\"), lit(0)) # <--- CORRECTION APPLIQU√âE ICI\n",
    "                                       )\n",
    "    \n",
    "    # PR(new) = (1 - alpha) * PR(contribution) + alpha / N\n",
    "    N = float(current_vertices.count())\n",
    "    \n",
    "    # Utiliser la colonne propre \"sum_contribution_clean\"\n",
    "    current_vertices = current_vertices.withColumn(\n",
    "        \"pagerank\", \n",
    "        lit(1.0 - RESET_PROBABILITY) * col(\"sum_contribution_clean\") + lit(RESET_PROBABILITY / N)\n",
    "    ).select(\"id\", \"label\", \"pagerank\").cache()\n",
    "    \n",
    "    print(f\"It√©ration {i+1} compl√©t√©e.\")\n",
    "    \n",
    "results = current_vertices.orderBy(col(\"pagerank\").desc())\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23fb66f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 10 des clients FRUDULEUX (label=1) avec le PageRank le plus √©lev√© ---\n",
      "+-------------------+-----+------------------+\n",
      "|                 id|label|          pagerank|\n",
      "+-------------------+-----+------------------+\n",
      "|  -1.58550536691994|    1|0.4437073305244993|\n",
      "|  -3.49910753739178|    1|0.4437073305244993|\n",
      "|0.00843036489558254|    1|0.4437073305244993|\n",
      "|  -0.25147096006823|    1|0.4437073305244993|\n",
      "|  0.725645739819857|    1|0.4437073305244993|\n",
      "| 0.0267792264491516|    1|0.4437073305244993|\n",
      "|  -1.81328048476897|    1|0.4437073305244993|\n",
      "|  0.314596589729515|    1|0.4437073305244993|\n",
      "|  0.857321003765953|    1|0.4437073305244993|\n",
      "|  -1.78322883722709|    1|0.4437073305244993|\n",
      "+-------------------+-----+------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 01:35:07 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-58abbaaa-f3b8-41f6-ab22-6bd008b8cef3. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-58abbaaa-f3b8-41f6-ab22-6bd008b8cef3\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:199)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:116)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1048)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:372)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:368)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:363)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2166)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:118)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2395)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2395)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2297)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$36(SparkContext.scala:704)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:231)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:205)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:205)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:205)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:184)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "# üîπ √âtape 7 : Clients frauduleux les plus centraux (R√©sultat final)\n",
    "print(\"--- Top 10 des clients FRUDULEUX (label=1) avec le PageRank le plus √©lev√© ---\")\n",
    "suspect = results.filter(col(\"label\") == 1).orderBy(col(\"pagerank\").desc())\n",
    "suspect.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea0ff2-4d32-4f95-ae9e-93766fbec5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
