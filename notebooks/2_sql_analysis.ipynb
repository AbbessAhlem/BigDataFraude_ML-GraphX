{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdefb5f",
   "metadata": {},
   "source": [
    "# Notebook 2 - SQL Analysis\n",
    "**Objectif :** Effectuer des agrÃ©gations exploratoires sur les donnÃ©es originales (non transformÃ©es) via Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b60dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/05 01:01:35 WARN Utils: Your hostname, TUF-GAMING-FX504GD, resolves to a loopback address: 127.0.1.1; using 192.168.1.145 instead (on interface wlo1)\n",
      "26/01/05 01:01:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/05 01:01:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”¹ Ã‰tape 1 : Initialisation Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, stddev\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fraud-SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db792a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                        (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 01:01:43 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”¹ Ã‰tape 2 : Charger le dataset CSV original et crÃ©er la vue temporaire (CORRECTION)\n",
    "# Nous chargeons le CSV pour garantir la prÃ©sence des colonnes 'Amount' et 'Class' nÃ©cessaires Ã  l'analyse exploratoire.\n",
    "train_df = spark.read.csv(\"hdfs:///user/hadoop/BigDataFraude_ML-GraphX/creditcard.csv\", header=True, inferSchema=True)\n",
    "train_df.createOrReplaceTempView(\"train_transactions\")\n",
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c876168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Nombre de transactions par classe ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                        (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|   492|\n",
      "|    0|284315|\n",
      "+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ðŸ”¹ Ã‰tape 3 : Nombre de transactions par classe\n",
    "print(\"--- Nombre de transactions par classe ---\")\n",
    "# 'Class' est rÃ©solue car nous chargeons le CSV original\n",
    "df_class_count = spark.sql(\"SELECT Class AS label, COUNT(*) AS count FROM train_transactions GROUP BY Class\")\n",
    "df_class_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806b6642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Montant moyen et Ã‰cart-type par classe ---\n",
      "+-----+-----------------+------------------+\n",
      "|label|       avg_amount|     stddev_amount|\n",
      "+-----+-----------------+------------------+\n",
      "|    1|122.2113211382114| 256.6832882977121|\n",
      "|    0|88.29102242231271|250.10509222589235|\n",
      "+-----+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”¹ Ã‰tape 4 : Montant moyen et Ã‰cart-type par classe\n",
    "print(\"\\n--- Montant moyen et Ã‰cart-type par classe ---\")\n",
    "# 'Amount' est rÃ©solue et permet le calcul de la moyenne et de l'Ã©cart-type\n",
    "df_avg_amount = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Class AS label, \n",
    "        AVG(Amount) AS avg_amount,\n",
    "        STDDEV(Amount) AS stddev_amount\n",
    "    FROM train_transactions \n",
    "    GROUP BY Class\n",
    "\"\"\")\n",
    "df_avg_amount.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
